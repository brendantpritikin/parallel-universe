#include <math.h> // include math library
#include <stdio.h> // inlcude standard IO library
#include <stdlib.h> //include standard library
#include <time.h> // include time library.
#include <mpi.h> // include MPI library.


/*
 * This Computer Science Senior Capstone Project C file leverages 
 * high-performance computing (HPC) techniques via the Message-Passing 
 * Interface (MPI) to attempt to increase and monitor CPU temperatures 
 * to maintain a specific overall temperature of a node cluster.
 *
 * This software is being written for a small-scale Raspberry Pi node cluster 
 * in a proof-of-concept format, with the knowledge that each node has a quad-core
 * CPU.
 * 
 * Data collected will be visualized to extrapolate on the potential of using a 
 * computing cluster, such as a data center, to provide a relatively-consistent 
 * baseline-level of heat output for productive re-use, while hopefully not 
 * significantly impacting the performance of the data center itself. Percentage of
 * acceptable temperature fluctuation is yet-to-be-determined.
 * 
 * CPUs will be maxed-out through a custom calculation function to keep CPUs loaded
 * with jobs, and temperature data will be recorded after each calculation. A goal 
 * is to set a temperature limit on the CPUs to pause calculations when temperatures 
 * veer outside of the acceptable range. This would allow for a predictable temperature
 * output range under maximum CPU load, regardless of requested processing power by outside
 * network clients. This, in turn, would demonstrate practicality by way of a predictable 
 * temperature output in the use of this concept as a real-world heating tool.
 * 
 * Data will be visualized using the Ganglia open-source software package.
 * 
 */


/* Author: Brendan T. Pritikin */
/* Thesis Advisor: Prof. John Rieffel */
/* Institution: Union College, Schenectady, NY */
/* Last Modified: February 25, 2022 */


/**
 * @brief squares two ints.
 * 
 * @param num_to_square number to square.
 * @return int squared value.
 */
int square(int num_to_square)
{
    int squaredNum = (num_to_square * num_to_square);
    
    return squaredNum;
}


/**
 * @brief gets current CPU temp data from a single Raspberry Pi Node.
 * 
 * @return double current CPU temp for a single node.
 */
double get_CPU_Temp()
{
    FILE *temperatureRecord; // open Temp file from Pi.
    double currentTemperature; // core temperature data.
    temperatureRecord = fopen("/sys/class/thermal/thermal_zone0/temp", "r"); // read CPU temp. data from Pi.

    fscanf(temperatureRecord, "%lf", &currentTemperature);
    currentTemperature = (((currentTemperature / 1000) * 9) / 5); // convert from mili-degrees to Fahrenheit.

    return currentTemperature;
}

/**
 * @brief get temperature of each node's CPU, add to array.
 * 
 * @param number_of_cores_total 
 *  
 */
void save_all_node_temps(int number_of_cores_total, double** node_temp_record_array, int next_save_index_iteration)
{
    for(int core = 0; core < (number_of_cores_total-3); core+=4)
    {
        double currentTemperature = get_CPU_Temp();
        printf("Node %d CPU temp: %f\n", (core/4), currentTemperature);
        node_temp_record_array[next_save_index_iteration][core] = currentTemperature;
        printf("Temp saved.\n------\n"); 
    }
}



/**
 * @brief create an array ranging from the starting to the ending value passed-in (inclusive).
 * @return int* array full of values from start-to-finish.
 */
int * filled_value_array(int starting_value, int ending_value)
{
    static int int_array[4000];

    for(int current_index = starting_value; current_index < 4000; current_index++)
    {
        int_array[current_index] = current_index;
    }
    return int_array;
}


/**
 * @brief create an array to store node CPU temp recordings.
 * 
 * @param num_cores_total total cores, to use for sizing to # of nodes.
 * @param number_of_temp_recordings total recordings to capture for program run.
 * @return double* array to store data in.
 */
double * temperature_storage_array(int num_cores_total, int number_of_temp_recordings)
{
    static double temp_array[4000];
    return temp_array; // this should return a pointer to the beginning of the array.
}

/**
 * @brief stores result of MPI squaring operations. Filled with resultant squares squaring_range_start - squaring_range_end.
 * 
 * @param range_start first number to square (main-defined).
 * @param range_end last number to square (main-defined).
 * @return int* array from range_start to range_end
 */
int * resultant_data_storage_array(int range_start, int range_end)
{
    static int squaring_resultant_array[4000];
    return squaring_resultant_array; // this should return a pointer to the beginning of the array.
}



int main(int argc, char* argv[]){

//NOT IN USE (WILL DELETE)
//double randNum1; // for squaring fcn.
//double randNum2; // for squaring fcn.
// double squareResult; //stores squared value to run-up processor core temps via arbitrary work.
int introMessageDisplayed = 0; // allows display of informational message upon mpirun.
int num_of_temp_recordings = 1000; // number of temperature records to store for each node.
int current_recording_iteration = 0; // holds current iteration count of temp recording.

double* node_temp_record;
const double TEMPERATURE_THRESHOLD = 70.0; // maximum Â°F setting for CPUs, i.e. temperature limit.
int num_range_start = 0; //
//int MPI_next_index_to_square = num_range_start; // stores value set above for MPI-specific use (just to keep things separate).
int num_range_end = num_of_temp_recordings; // enough room to hold the number of recordings planned.
// SQUARING RANGE START - SQUARING RANGE END MUST == N/P (tasks/processes). 
int* resultant_data_storage = resultant_data_storage_array(num_range_start, num_range_end); //holds all resulting squares calculated across nodes.
int* values_array_filled = filled_value_array(num_range_start, num_range_end);
int values_array_size = 4000; //(num_range_end - num_range_start);
int* final_data_array = resultant_data_storage_array(num_range_start, num_range_end); // holds received values. from MPI_Recv.


//RESULTANT DATA STORAGE ARRAY MALLOC: (int *) malloc((range_end - range_start) * sizeof(int)); // 1-D array for holding squared value resultants.
//FILLED VALUE ARRAY MALLOC: (int *) malloc(starting_value * ending_value * sizeof(int)); // holds all ints to be loaded in, start-to-finish.
//TEMPERATURE STORAGE ARRAY (just for the heck of it as it has a malloc): (double *) malloc(number_of_temp_recordings * (num_cores_total/4) * sizeof(double)); //holds each node's temps in a 2D array.
//SETTING ALL TO A SIZE OF 4000.



// initialize MPI
MPI_Init(&argc, &argv); // sets up MPI. Do not alter.

int number_of_cores; // for MPI testing.
int core_number; // for MPI testing.
char system_name[MPI_MAX_PROCESSOR_NAME]; // for MPI testing.
int sys_name_char_length; // for MPI testing

//UNUSED. WILL DELETE. NOT SQUARING RANDOM NUMBERS, SQUARING NUMBERS CHRONOLOGICALLY FROM 1-1000, IN THIS EXAMPLE.
//randNum1 = ((unsigned)(time(NULL))); // get unix timestamp for first random number.
//randNum2 = rand() % (100 + 1 - 1) + 1; // generate "random" number between 1 and 100. (.. % max_num + 1 - min_num) + min_num.
//double currentTemperature;


//BEGIN MPI TESTING.

//recall, everything in this program will be run once by each
// processor, including all print statements.

//Get basic data about system
// 1. Number of processes (cores/processors available).
// 2. Rank of processors.
// 3. Name of processors.
// 4. Current temperature of processors (raspberry pi-specific).

MPI_Comm_size(MPI_COMM_WORLD, &number_of_cores); // get number of processes.
MPI_Comm_rank(MPI_COMM_WORLD, &core_number); // get rank of each processor (processor #)
MPI_Get_processor_name(system_name, &sys_name_char_length);


// Test hello message
printf("\n\nhello from the computer named %s, core %d out of %d.\n\n", 
            system_name, core_number, number_of_cores);



//END MPI TESTING.

//#FIXME - ADD MPI collective-communication for increasing efficiency in calculations...
//#FIXME - perhaps have an array of values from 1 to number_of_cores, and instruct each core to square the value in
//#FIXME - its individual node index until 1,000 operations have occurred?
    //YES - ARRAY IS CALLED RESULTANT_DATA_STORAGE_ARRAY. IT IS 1-D.    

// So, working idea now is to implement the following: the calculation we can do with MPI_Recv (and MPI_Send) is given a required calculation over a
// certain distance/range (say, collectively we want a list of every square from 1-1,000,000,000 [inclusive]). To accomplish this more efficiently than serially,
// we can pass work split up in the format of n/p (tasks/processors available)
// to every available process discovered at MPI_Init time. 
//So, we take the number of tasks (1,000,000,000 - 1 tasks) and the number of processes P
// (which, for any dual-core or higher system will be evenly-divisible), separate out the work by # of tasks, evenly,
// compute, and return as the values are calculated. Given MPI_Recv and MPI_Send, processes will return as they have completed,
// not in an orderly fashion, but that's OK. Goal is to compute everything REQUIRED as QUICKLY as possible, with the benefit
// of parallel computing to accelerate an otherwise slow, slow task.


if(introMessageDisplayed == 0 && core_number == 0){
    printf("-----------------------------------------\n");
    printf("PROGRAM INFORMATION: \n\nLoaded: \n1. math.h\n2. stdio.h\n3. stdlib.h\n4. time.h\n5. mpi.h\n\n");

    printf("This program will calculate two pseudo-random numbers and will then square them.\n");
    printf("The process will continue until a CPU temperature threshold has been set,\n");
    printf("at which time the program will PAUSE until the temperature has fallen below\n");
    printf("the threshold. Then, squaring of numbers will continue.\n");
    printf("-----------------------------------------\n\n\n\n\n");

    introMessageDisplayed = 1;
}


printf("\n------------------------------------------\n");

// create storage array for node temperature data.
node_temp_record = temperature_storage_array(number_of_cores, num_of_temp_recordings);


int i;

for (i = num_range_start; i < values_array_size; i++)
{
    resultant_data_storage[i] = square(values_array_filled[i]); //square VAF and store in RDS.
    //printf("I'm core %d!\n", core_number); works.
}

printf("squared value by process %d is %d.\n", core_number, resultant_data_storage[i]);



/**
 * @brief MPI_Send task/values_array_fllled values FROM all non-zero nodes TO node 0
 * for calculations to take place.
 * 
 */
for(int core = 4; core < (number_of_cores/4); core+=4)
{
    MPI_Send(resultant_data_storage, 1, MPI_INT, 0, 0, MPI_COMM_WORLD); // MPI_Send-to-core-0-operation.
    printf("Number two! I'm core %d!\n", core_number);
}

/**
 * @brief MPI_Receive tasks from all non-zero nodes to node 0 (assuming quad-core).
 *          Goal: square next value in values_array_filled and store result in 
 *                  resultant_data_storage.
 * 
 */
if(core_number == 0)
{
    MPI_Recv(final_data_array, 1, MPI_INT, core_number, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    printf("Finally, I'm core %d!\n", core_number);
}









/**


if(core_number == 0) //node 0,core1 i.e. first core in first CPU.
{
    printf("OLD hard-coding FOR TESTING = node 0 CPU temp: \n\n");
    currentTemperature = get_CPU_Temp();
    printf("%f", currentTemperature);
    printf("\n------------------------------------------\n");
}

if(core_number == 4) //node 1
{
    printf("OLD hard-coding FOR TESTING = node 1 CPU temp: \n\n");
    currentTemperature = get_CPU_Temp();
    printf("%f", currentTemperature);
    printf("\n------------------------------------------\n");
}

if(core_number == 8) // node 2
{
    printf("OLD hard-coding FOR TESTING = node 2 CPU temp: \n\n");
    currentTemperature = get_CPU_Temp();
    printf("%f", currentTemperature);
    printf("\n------------------------------------------\n");
}

*/

/**
 * @brief save currrent CPU temperatures from all connected nodes.
 * 
 */
if(current_recording_iteration < num_of_temp_recordings)
{
    save_all_node_temps(number_of_cores, &node_temp_record, current_recording_iteration);
}

for(int core = 0; core < number_of_cores-3; core+=4)
{
    // [core][current_recording_iteration] old array-iterating code.
    //row-major format - i = current iteration/row; ncols = CPU # (assuming quad-core cpu); j = column/cpu temp.
    while(node_temp_record[current_recording_iteration * (number_of_cores/4) + core]  > TEMPERATURE_THRESHOLD)
    {
        printf("SUCCESS. TEMPERATURE THRESHOLD REACHED. Waiting for CPUs to cool\n");
        printf("just below threshold before continuing calcuations to maintain a consistent average temperature.");
        printf("CPU %d temp is %f and limit is set at %f.", core, node_temp_record[current_recording_iteration * (number_of_cores/4) + core], TEMPERATURE_THRESHOLD);
    }
} 


//#FIXME: OLD CODE call/response from cores. WILL DELETE AFTER TESTING NEW CODE.

/**

if(core_number == 0)
{
    printf("test message from core 0.");
}

if(core_number == 1)
{
    printf("test message from core 1.");
}

if(core_number == 2)
{
    printf("test message from core 2.");
}

if(core_number == 3)
{
    printf("test message from core 3.");
}

*/

if(core_number == 0)
{
//    printf("\n\nFilled value array looks like this: %d.", *values_array_filled); // numbers before squaring.
//    printf("\n\nresulting data looks like this: %d", *final_data_array); //squared number results.
    printf("\n\nHere is the array of collected temperature data: %f.\n", *node_temp_record); //node temp data.
}

//free(node_temp_record); //not necessary unless using malloc().
//free(values_array_filled); //not necessary unless using malloc().
//free(resultant_data_storage); //not necessary unless using malloc().
//free(final_data_array); //not necessary unless using malloc().

MPI_Finalize(); //Finalize MPI environment.

return 0;

}
